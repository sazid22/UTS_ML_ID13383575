{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_13383575.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sazid22/UTS_ML_ID13383575/blob/master/A1_13383575.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjYfWKocU0Au",
        "colab_type": "text"
      },
      "source": [
        "# Review Report on Generative Adversarial Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztuci5wPiOQ7",
        "colab_type": "text"
      },
      "source": [
        "##Student Information:\n",
        "###Name - Sazid Al Ahsan\n",
        "###Student ID - 13383575\n",
        "###Github Link of Repository - https://github.com/sazid22/UTS_ML_ID13383575\n",
        "###Colab Link - https://colab.research.google.com/drive/1SCzuvfy7KpyRn-0tM2p_h9DcrSALZ1mV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI3u8CcHRTdm",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "The following report is on the paper \"Generative Adversarial Net\" by Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville and Yoshua Bengio. This paper was published in 2014 which appers to be the founding block of research on adversarial learning. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x7ueUjlUoW5",
        "colab_type": "text"
      },
      "source": [
        "## Content\n",
        "In the paper \"Generative Adversarial Network\", the authors have proposed a new model that has opened new possibilities in the research of generative modeling. Discriminative models have been a popular research topic in the field of machine learning, but this is the leading publication on generative models using the adversarial process (Goodfellow et al. 2014). What a generative model does is they create new data points from given training data points. Previously it was done by mapping from X to Y where X is the training data space and Y is the corresponding data points in the generated data space. This paper provides a new way of generating this fake dataspace. Rather than doing mapping, the authors provide a framework consisting of a generative model and a discriminative model. The generator creates datapoint given any distribution and discriminator identifies if the data is from a real distribution or it has been fakely generated. This is where the term \"adversarial\" comes handy, as discriminator identifies between the fake and real, the generator produces better output through the process by learning on the basis of the outcome of the discriminator. The goal of this generative model is as such that the discriminator predicts as low as possible approximately half of the fake points and the real point.  \n",
        "\n",
        "As mentioned earlier, the generative model has been a field of interest for machine learning pioneer, there has been some issue. This is mostly with the probability approximation where it might require to use less accurate maximum likelihood estimation or models like Markov chains is used to calculate the probability. This makes the approximation very complex as well as inaccurate. Ian Goodfellow and his team stepsides these difficulties and propose a new process for the estimation calculation of the generative model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggf4Mi4Wf2v2",
        "colab_type": "text"
      },
      "source": [
        "##Innovation\n",
        "This research on generative modeling is very innovative for various reasons. \n",
        "\n",
        "*   As mentioned, the earlier papers relied on maximizing the likelihood, this paper proposes the idea of adversarial training. Whereas previous generative models for example deep directed graphical models are not able to calculate the probability distribution of the dataset, p(x) and later using error-prone estimation methods, GAN completely avoids this explicit estimation. Rather it uses binary classification feedback from the discriminator to train the generator.  \n",
        "*  Whereas the previous models used a method like Markov Chains to do estimation, this also made the process slow and computationally complex. This paper uses the feed-forward network which works from latent space Z to data space X to create fake data which is much simpler than those of the previous models. \n",
        "*   As the proposed model is a feedforward model, the generation is not dependent on observations from previous time steps. As a result, the GAN model can parallelize the generation process which makes it faster. Auto-Regressive Models like PixelCNN where they work on an initially imposed model are not able to do this parallel generation(Theis, Oord & Bethge 2015). \n",
        "*   This paper introduces two loss functions. One is Minimax loss and another is non-saturated loss. The later is advised by the authors to practice and they implemented the later in this paper. What non-saturated loss is that rather than the data generated having a high probability of being real, the generator tries to decrease the alternative possibility (Hong et al. 2019).\n",
        "\n",
        "*    This paper is innovative for the application of generative models. It has shown the direction in many fields where generative models can be used and help improve the existing field. From artistic use to field like medical images GAN has provided a new direction. As it is computationally less complex and faster, GAN can perform better and less error-prone than existing generative models before that. \n",
        "\n",
        "Finally, this paper has paved a new direction for researchers among scholars. There have been numerous publications and repositories on GAN and its variants. It can be said that with existing works on generative models, this paper contributes in a great way in the field of machine learning with its innovation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48UjtNXLO-Og",
        "colab_type": "text"
      },
      "source": [
        "## Technical Quality\n",
        "This paper was published in NIPS 2014, which is one of the leading conferences on machine learning. Overall the paper has high technical quality. The description of the model was very well described. They have explained their novelty with a good explanation. Mathematical equations were provided so that it is easy to grasp the idea of their hypothesis as well as the provided proof of these equations. They did not limit this proof with their own findings but provided comparisons with existing divergence models and explained how their model is less complex and computationally superior than the existing generative model. In the paper, they provided an algorithm and pseudocode of this algorithm which makes the way for implementing the model. They also derived two propositions from the algorithm and showed their proof mathematically using existing divergence methods and described the proposition's optimality.\n",
        "\n",
        "Even though the theoretical part of this paper is good, there are issues with an experiment conducted in parallel to technical writing. The number of experiment conducted is only 3 and these were conducted in a very limited environment. Even the comparison was not shown for all 3 experiments, as the authors went to show a comparison between 2 experiments. So this arises a reliability issue of the model proposed. Another issue with the paper is that they used log-likelihood estimation to evaluate their methods. Log-likelihood has a tendency to show high variance in the result which arises questions in terms of reliability. \n",
        "\n",
        "Another point to mention is that the authors went for only qualitative comparison. Qualitative can be idiosyncratic sometimes as it varies from the experimental setup. But the authors have mentioned this issue and they have mentioned they went for qualitative comparison to show that GANs are on par with the existing generative models in terms of complexity and speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVBjDmGl2TxF",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor\n",
        "The application domain of GAN has not been described in this paper. But it is pretty admissible that GANs can be used in any type of application where the generative model is used. Since the publication of this paper, there has been a lot of research and implementation where we can see the application of that. Progressive-Growing GANs is one of the first GAN models that has been used commercially (Karras et al. 2017). This GAN architecture was released by Nvidia which shows great results in image synthesis. What Progressive-Growing GAN does is it can create very high-resolution fake images. It was a big jump from low-resolution images like CIFAR-10 where the resolution is $32^2$ whereas this architecture can generate a facial image that has a resolution of $1024^2$.\n",
        "\n",
        "GAN can also generate videos. This project is done by MIT students. They taught two neural networks over two million unlabeled videos (Vondrick, Pirsiavash & Torralba 2016). What their architecture does is given any image, it can generate the next 2 seconds. What they did was base on a good intention, but GAN has been used in creating fake videos later on. Last year Deepfake videos were made with an intention to create a hoax and fake news. This shows GANs can be used to gain spiteful purposes.\n",
        "\n",
        "In a brief, this paper has introduced scholars with a great opportunity to work on adversarial learning. Generative models have been there but a new door was opened for everybody. The amount of papers, actually great quality papers on GANs being published since 2014 has increased each year. Various kind of GAN architecture for different purposes has been introduced. Not just variants, optimization of GAN has also been a topic of works for researchers. Semi-supervised learning has become a point of the extensive researcher after authors of this paper proposed as future work. Moreover, many well-known universities have begun courses on GAN and big companies are also working on implementing GAN in various applications and research. So it can be said GAN has created a lot of buzz in the world of machine learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1ysgt8NZ8Tj",
        "colab_type": "text"
      },
      "source": [
        "##Presentation\n",
        "The overall presentation of this paper is very good. The paper does not have a vague idea and the representation is succinct. The abstract of the paper provides the idea of the paper very clearly without any jargon and represents the idea of what this paper proposes. The introduction provides us with what they are going to do and more formally presents the novelty of the paper. In the literature review section, they discussed existing methods of generative models and their disadvantages and how their work is going to overcome these. In the next section, \"Adversarial Nets\", the authors gave a brief introduction of adversarial learning and provided the mathematical equation of that which is indicative of a good presentation goal. The next section, \"Theoretical Result\" is the strongest part of this paper. The provided algorithm how the method will run then they provide theorem and propositions over them and has provided the proof of those which are really great and mathematically very intuitive. Then they showed their experiment. It was a weak part of the paper as they practically implemented the idea in a very narrow way and only three experiments were done. So it was a weak part of the paper but as an inaugural paper on adversarial learning, it somehow can be overlooked and taken as a learning for future experiments. Lastly, they provided an open-ended idea for future works and the time my report is being written I can see on the internet a lot of those ideas are now being worked on. The authors also provided the Github repository of all their code but there is a little description of those codes and the repository was not well maintained. But overall this is a great paper and provides direction for machine learning practitioners. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T659Xg3Gs6aS",
        "colab_type": "text"
      },
      "source": [
        "##References\n",
        "\n",
        "\n",
        "*   Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. & Bengio, Y. 2014, 'Generative adversarial nets', Advances in neural information processing systems, pp. 2672-80.\n",
        "*  Hong, Y., Hwang, U., Yoo, J. & Yoon, S. 2019, 'How Generative Adversarial Networks and Their Variants Work: An Overview', ACM Comput. Surv., vol. 52, no. 1, pp. 1-43.\n",
        "*  Karras, T., Aila, T., Laine, S. & Lehtinen, J. 2017, 'Progressive growing of gans for improved quality, stability, and variation', arXiv preprint arXiv:1710.10196.\n",
        "*   Theis, L., Oord, A.v.d. & Bethge, M. 2015, 'A note on the evaluation of generative models', arXiv preprint arXiv:1511.01844.\n",
        "\n",
        "*   Vondrick, C., Pirsiavash, H. & Torralba, A. 2016, 'Generating videos with scene dynamics', paper presented to the Proceedings of the 30th International Conference on Neural Information Processing Systems, Barcelona, Spain.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLIUbEWwu6_N",
        "colab_type": "text"
      },
      "source": [
        "###The following code was taken from Github. I am not able to provide the repository as it was copied around 20 days ago. This code is about the data dribution using GAN, in reference to introduction to adversarial Nets part of the part where the authors discuss about latent space and data space. And the minimax game function is implemented in the following code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-7XfSPpu4rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "from scipy.stats import norm\n",
        "\n",
        "style.use('ggplot')\n",
        "\n",
        "# Input Data Sample (P_data)\n",
        "mu, sigma = -1,1\n",
        "xs=np.linspace(-5,5,1000)\n",
        "# plt.plot(xs, norm.pdf(xs,loc=mu,scale=sigma))\n",
        "\n",
        "numTrainIters = 10000\n",
        "M = 200\n",
        "\n",
        "# ------------------------------ Multi Layer Perceptron -------------------------\n",
        "# MultiLayer Perceptron Layers Weights and Bias Value Initialization\n",
        "# 4-layer ANN with 1 Hidden Layer (6 neurons) and 1 Deeply Connected Layer (5 neurons)\n",
        "# Total Number of Neurons = 11\n",
        "# Input => Hidden Layer => Deeply Connected Layer => Output\n",
        "# Weights are between the layers.\n",
        "# Biases are on the neurons of the Hidden Layers.\n",
        "def MultiLayerPerceptron(input, output_dim):\n",
        "    # Initialize the weights with \"empty\"\n",
        "    # Initialize the biases with \"0.0\"\n",
        "    init_const = tf.constant_initializer(0.0)\n",
        "    init_normal = tf.random_normal_initializer()\n",
        "    # Format: tf.get_variable(name, shape, datatype, initializer, regularizer...)\n",
        "    # Weights from Input layer to Hidden Layer => W1\n",
        "    # W1 matrix is of the form: [Input data samples, number of neurons in Hidden Layer]\n",
        "    w1 = tf.get_variable('w1', [input.get_shape()[1], 6], initializer=init_normal)\n",
        "\n",
        "    # Bias is added on the Hidden Layer Neuron.\n",
        "    # Dimension of Bias Matrix: [number of neurons in hidden layer]\n",
        "    b1 = tf.get_variable('b1', [6], initializer=init_const)\n",
        "\n",
        "    # Deeply Connected Layer / Second Hidden Layer\n",
        "    # Weight Dimension: [num neurons in 1st Hidden Layer, num neurons in 2nd Hidden Layer]\n",
        "    w2 = tf.get_variable('w2', [6, 5], initializer=init_normal)\n",
        "\n",
        "    # Bias is added on the Hidden Layer Neuron.\n",
        "    # Dimension of Bias Matrix: [number of neurons in 2nd Hidden Layer]\n",
        "    b2 = tf.get_variable('b2', [5], initializer=init_const)\n",
        "\n",
        "    # Weights from Deeply Connected Layer to Output Neuron / Layer\n",
        "    # W3 matrix has dimensions: [num neurons in hiddenlayer - 1, num neurons in output layer]\n",
        "    w3 = tf.get_variable('w3', [5, output_dim], initializer=init_normal)\n",
        "\n",
        "    # Bias is added on the Output Layer.\n",
        "    # Dimension of Bias Matrix: [number of neurons in Output Layer]\n",
        "    b3 = tf.get_variable('b3', [output_dim], initializer=init_const)\n",
        "\n",
        "    # Activation Function (tanh()) for 1st Hidden Layer\n",
        "    a1 = tf.nn.tanh(tf.matmul(input, w1) + b1)\n",
        "\n",
        "    # Activation Function (tanh()) for 2nd Hidden Layer\n",
        "    a2 = tf.nn.tanh(tf.matmul(a1, w2) + b2)\n",
        "\n",
        "    # Activation Function (tanh()) for Output Layer\n",
        "    y_Hat = tf.nn.tanh(tf.matmul(a2, w3) + b3)\n",
        "    return y_Hat, [w1, b1, w2, b2, w3, b3]\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------- Train the Neural Network ---------------------------\n",
        "# Using Momentum Optimizer to decrease the Learning Rate as\n",
        "# we progress so as to reach the minima and not miss it.\n",
        "# Useful in case when the input is not a Convex function.\n",
        "def momentumOptimizer(loss, var_list):\n",
        "    # Initialize the learning rate to a reasonable value\n",
        "    baseLearningRate = 0.001\n",
        "    # Set the decay Rate\n",
        "    decayRate = 0.95\n",
        "    # Set the step size for decay\n",
        "    decaySteps = numTrainIters // 4\n",
        "    batch = tf.Variable(0)\n",
        "    learning_rate = tf.train.exponential_decay(\n",
        "        baseLearningRate,\n",
        "        batch,\n",
        "        decaySteps,\n",
        "        decayRate,\n",
        "        staircase=True\n",
        "    )\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate, 0.6).minimize(\n",
        "        loss,\n",
        "        global_step=batch,\n",
        "        var_list=var_list\n",
        "    )\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "# ------------------------ Generative Adversarial Network --------------------------\n",
        "# Pre-Train the Discriminator.\n",
        "# Doing this saves time at a later stage and only tuning of Discriminator is required.\n",
        "with tf.variable_scope(\"D_pre\"):\n",
        "    # Input Data to the Discriminator\n",
        "    # N sample values in matrix form\n",
        "    input_node = tf.placeholder(tf.float32, shape=(M, 1))\n",
        "\n",
        "    # Training Labels = Number of input samples\n",
        "    train_labels = tf.placeholder(tf.float32, shape=(M, 1))\n",
        "\n",
        "    # Input the data to Neural Network and get the Output\n",
        "    # Discriminator sample (D) = Output (y_Hat)\n",
        "    # Neural Network[input_node, hidden_dim = 6, output_dim = 1]\n",
        "    D, theta = MultiLayerPerceptron(input_node, 1)\n",
        "\n",
        "    # Calculate the loss using Mean Squared Error (MSE)\n",
        "    loss = tf.reduce_mean(tf.square(D - train_labels))\n",
        "\n",
        "# Optimize the Discriminator for Minimizing Loss / Cost Function.\n",
        "optimizer = momentumOptimizer(loss, None)\n",
        "\n",
        "\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "\n",
        "\n",
        "# ------------------------ Plot Initial Decision Surface -----------------------------\n",
        "def plot_d0(D, input_node):\n",
        "    f, ax = plt.subplots(1)\n",
        "    # p_data\n",
        "    xs = np.linspace(-5, 5, 1000)\n",
        "    ax.plot(xs, norm.pdf(xs, loc=mu, scale=sigma), label='p_data')\n",
        "    # decision boundary\n",
        "    r = 1000  # resolution (number of points)\n",
        "    xs = np.linspace(-5, 5, r)\n",
        "    ds = np.zeros((r, 1))  # decision surface\n",
        "    # process multiple points in parallel in a minibatch\n",
        "    for i in range(int(r/M)):\n",
        "        x = np.reshape(xs[M*i:M*(i + 1)],(M,1))\n",
        "        ds[M*i:M*(i + 1)] = sess.run(D, {input_node: x})\n",
        "\n",
        "    ax.plot(xs, ds, label='decision boundary')\n",
        "    ax.set_ylim(0, 1.1)\n",
        "    plt.legend()\n",
        "\n",
        "# plot_d0(D, input_node)\n",
        "# plt.title('Initial Decision Boundary')\n",
        "# plt.show()\n",
        "# sess.close()\n",
        "\n",
        "\n",
        "# ------------------------------------ Plot Training Loss -----------------------------\n",
        "lh=np.zeros(1000)\n",
        "for i in range(1000):\n",
        "    #d=np.random.normal(mu,sigma,M)\n",
        "    d=(np.random.random(M)-0.5) * 10.0 # instead of sampling only from gaussian, want the domain to be covered as uniformly as possible\n",
        "    labels=norm.pdf(d,loc=mu,scale=sigma)\n",
        "    lh[i],_=sess.run([loss,optimizer], {input_node: np.reshape(d,(M,1)), train_labels: np.reshape(labels,(M,1))})\n",
        "\n",
        "# plt.plot(lh)\n",
        "# plt.title('Training Loss')\n",
        "# plt.show()\n",
        "\n",
        "# plot_d0(D,input_node)\n",
        "\n",
        "# Theta gets the weights and biases with lowest cost function for Pre Trained Network D.\n",
        "# Save it.\n",
        "learnedWeights = sess.run(theta)\n",
        "sess.close()\n",
        "\n",
        "\n",
        "# Generator Network\n",
        "with tf.variable_scope(\"G\"):\n",
        "    # M: Input Samples generated (Noise Signal)\n",
        "    z_node=tf.placeholder(tf.float32, shape=(M,1))\n",
        "    # Get the output and weights of the Generator using Neural Network\n",
        "    G,theta_g = MultiLayerPerceptron(z_node,1)\n",
        "    # Scale by 5 to match with range\n",
        "    G=tf.multiply(5.0,G)\n",
        "\n",
        "\n",
        "# Discriminator Network:\n",
        "# D1 => actual Data input Discriminator\n",
        "# D2 => Output of Generator is Input to Discriminator\n",
        "with tf.variable_scope(\"D\") as scope:\n",
        "    # D1 => Trained on Input Data\n",
        "    x_node = tf.placeholder(tf.float32, shape=(M,1))\n",
        "    fc,theta_d = MultiLayerPerceptron(x_node,1)\n",
        "    D1=tf.maximum(tf.minimum(fc,.99), 0.01)\n",
        "    # make a copy of D that uses the same variables, but takes in G as input\n",
        "    scope.reuse_variables()\n",
        "\n",
        "    # D2 => Takes output of Generator as Input\n",
        "    # Output of Generator (G) input to Discriminator (D2)\n",
        "    fc,theta_d = MultiLayerPerceptron(G,1)\n",
        "    D2 = tf.maximum(tf.minimum(fc,.99), 0.01)\n",
        "\n",
        "# Calculating the Value of Discriminator \"D\"\n",
        "# log(D1(x)) + log(1-D2(x))\n",
        "# We need to Maximize D1 and Minimize value of D2.\n",
        "# obj_d: Value receieved after processing Input Data\n",
        "obj_d = tf.reduce_mean(tf.log(D1)+tf.log(1-D2))\n",
        "\n",
        "# Calculating value of Generator Function \"G\"\n",
        "# log(D2(x))\n",
        "# We need to maximize log(D2(x)) to successfully fool Discriminator\n",
        "# obj_g: Value received after processing Generator Data\n",
        "obj_g = tf.reduce_mean(tf.log(D2))\n",
        "\n",
        "# set up optimizer for G and D\n",
        "opt_d = momentumOptimizer(1-obj_d, theta_d)\n",
        "opt_g = momentumOptimizer(1-obj_g, theta_g)\n",
        "\n",
        "\n",
        "\n",
        "sess=tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "\n",
        "\n",
        "\n",
        "# Copy the Weights saved in Pre-Training over to New D Network\n",
        "for i,v in enumerate(theta_d):\n",
        "    sess.run(v.assign(learnedWeights[i]))\n",
        "\n",
        "\n",
        "\n",
        "def plot_fig():\n",
        "    # plots pg, pdata, decision boundary\n",
        "    f,ax=plt.subplots(1)\n",
        "    # p_data\n",
        "    xs=np.linspace(-5,5,1000)\n",
        "    ax.plot(xs, norm.pdf(xs,loc=mu,scale=sigma), label='p_data')\n",
        "\n",
        "    # decision boundary\n",
        "    r=5000 # resolution (number of points)\n",
        "    xs=np.linspace(-5,5,r)\n",
        "    ds=np.zeros((r,1)) # decision surface\n",
        "    # process multiple points in parallel in same minibatch\n",
        "    for i in range(int(r/M)):\n",
        "        x=np.reshape(xs[M*i:M*(i+1)],(M,1))\n",
        "        ds[M*i:M*(i+1)]=sess.run(D1,{x_node: x})\n",
        "\n",
        "    ax.plot(xs, ds, label='decision boundary')\n",
        "\n",
        "    # distribution of inverse-mapped points\n",
        "    zs=np.linspace(-5,5,r)\n",
        "    gs=np.zeros((r,1)) # generator function\n",
        "    for i in range(int(r/M)):\n",
        "        z=np.reshape(zs[M*i:M*(i+1)],(M,1))\n",
        "        gs[M*i:M*(i+1)]=sess.run(G,{z_node: z})\n",
        "    histc, edges = np.histogram(gs, bins = 10)\n",
        "    ax.plot(np.linspace(-5,5,10), histc/float(r), label='p_g')\n",
        "\n",
        "    ax.set_ylim(0,1.1)\n",
        "    plt.legend()\n",
        "\n",
        "plot_fig()\n",
        "plt.title('Before Training')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Algorithm 1 of Goodfellow et al 2014\n",
        "# Instead of optimizing with one pair (x,z) at a time, we update the gradient\n",
        "# based on the average of M loss gradients computed for M different (x,z) pairs.\n",
        "# The stochastic gradient estimated from a minibatch is closer to the true gradient\n",
        "# across the training data.\n",
        "k=1\n",
        "histd, histg= np.zeros(numTrainIters), np.zeros(numTrainIters)\n",
        "for i in range(numTrainIters):\n",
        "    for j in range(k):\n",
        "        x= np.random.normal(mu,sigma,M) # sampled m-batch from p_data\n",
        "        x.sort()\n",
        "        # Sample Batch Noise Input of Dimension \"M\"\n",
        "        # Input Noise signal \"Z\" streached from \"-1 to 1\" to \"-5 to 5\"\n",
        "        z= np.linspace(-5.0,5.0,M)+np.random.random(M)*0.01\n",
        "        histd[i],_ = sess.run([obj_d,opt_d], {x_node: np.reshape(x,(M,1)), z_node: np.reshape(z,(M,1))})\n",
        "    # Sample Noise Prior Signal Input\n",
        "    z= np.linspace(-5.0,5.0,M)+np.random.random(M)*0.01\n",
        "    # Update Generator\n",
        "    histg[i],_ = sess.run([obj_g,opt_g], {z_node: np.reshape(z,(M,1))})\n",
        "    if i % (numTrainIters//10) == 0:\n",
        "        print((float(i)/float(numTrainIters))*100)\n",
        "\n",
        "\n",
        "plt.plot(range(numTrainIters),histd, label='obj_d')\n",
        "plt.plot(range(numTrainIters), 1-histg, label='obj_g')\n",
        "plt.legend()\n",
        "\n",
        "plot_fig()\n",
        "plt.show()\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}